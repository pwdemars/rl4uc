{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incoming-liberia",
   "metadata": {},
   "source": [
    "# Getting Started with RL4UC\n",
    "\n",
    "This notebook will briefly describe how to use RL4UC. It will cover the following elements:\n",
    "\n",
    "- Making an environment\n",
    "- Interacting with the environment\n",
    "- Training and testing an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-house",
   "metadata": {},
   "source": [
    "## Making an Environment\n",
    "\n",
    "Environments are specified with just a handful of arguments. The key ones are: \n",
    "\n",
    "- `num_gen`: number of controllable generators (must be at least 5)\n",
    "- `dispatch_freq_mins`: length of settlement periods/frequency of decision-making (every 30 minutes by default)\n",
    "- `voll`: the 'value of lost load', determining the magnitude of the penalty for failing to meet demand\n",
    "- `arma_demand` and `arma_wind`: a dictionary specifying the details of the auto-regressive moving average processes determining the forecast errors for demand and wind\n",
    "- `usd_per_kgco2`: the cost in USD of emitting a kilogram of CO2\n",
    "\n",
    "You can manually create an environment with your specifications using `make_env()`, or you can create an environment from a json file using `make_env_from_json()`. The json which must be located in `data/envs`. There are already a few datasets in there for systems ranging from 5 to 30 generators. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-madrid",
   "metadata": {},
   "source": [
    "First we will create an environment manually. All the variables have default values, so we will just specify a few here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boring-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4uc.environment import make_env\n",
    "import numpy as np\n",
    "\n",
    "env = make_env(num_gen=10, dispatch_freq_mins=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-brave",
   "metadata": {},
   "source": [
    "The generator specifications are all derived from a single dataset, widely used in the UC literature, from: Kazarlis, S.A., Bakirtzis, A.G. and Petridis, V., 1996. A genetic algorithm solution to the unit commitment problem. IEEE transactions on power systems, 11(1), pp.83-92.\n",
    "\n",
    "For 10 generators, the paper specifies quadratic fuel cost curves, cold and hot start costs (RL4UC currently considers hot starts), minimum up/down times and minimum and maximum operating outputs. \n",
    "\n",
    "Here is the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sporting-platinum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_output</th>\n",
       "      <th>max_output</th>\n",
       "      <th>status</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>t_min_down</th>\n",
       "      <th>t_min_up</th>\n",
       "      <th>t_max_up</th>\n",
       "      <th>hot_cost</th>\n",
       "      <th>cold_cost</th>\n",
       "      <th>cold_hrs</th>\n",
       "      <th>min_fuel_cost</th>\n",
       "      <th>max_cost_per_mwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>455</td>\n",
       "      <td>96</td>\n",
       "      <td>0.00048</td>\n",
       "      <td>16.19</td>\n",
       "      <td>1000</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>1000000</td>\n",
       "      <td>4500</td>\n",
       "      <td>9000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>18.606202</td>\n",
       "      <td>22.928667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>455</td>\n",
       "      <td>96</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>17.26</td>\n",
       "      <td>970</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>1000000</td>\n",
       "      <td>5000</td>\n",
       "      <td>10000</td>\n",
       "      <td>1000000</td>\n",
       "      <td>19.532918</td>\n",
       "      <td>23.773167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>130</td>\n",
       "      <td>-60</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>16.60</td>\n",
       "      <td>700</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>1000000</td>\n",
       "      <td>550</td>\n",
       "      <td>1100</td>\n",
       "      <td>1000000</td>\n",
       "      <td>22.244615</td>\n",
       "      <td>51.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>130</td>\n",
       "      <td>-60</td>\n",
       "      <td>0.00211</td>\n",
       "      <td>16.50</td>\n",
       "      <td>680</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>1000000</td>\n",
       "      <td>560</td>\n",
       "      <td>1120</td>\n",
       "      <td>1000000</td>\n",
       "      <td>22.005069</td>\n",
       "      <td>50.542200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>162</td>\n",
       "      <td>-72</td>\n",
       "      <td>0.00398</td>\n",
       "      <td>19.70</td>\n",
       "      <td>450</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>1000000</td>\n",
       "      <td>900</td>\n",
       "      <td>1800</td>\n",
       "      <td>1000000</td>\n",
       "      <td>23.122538</td>\n",
       "      <td>37.799500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>80</td>\n",
       "      <td>-36</td>\n",
       "      <td>0.00712</td>\n",
       "      <td>22.26</td>\n",
       "      <td>370</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>1000000</td>\n",
       "      <td>170</td>\n",
       "      <td>340</td>\n",
       "      <td>1000000</td>\n",
       "      <td>27.454600</td>\n",
       "      <td>40.902400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>85</td>\n",
       "      <td>-36</td>\n",
       "      <td>0.00079</td>\n",
       "      <td>27.74</td>\n",
       "      <td>480</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>1000000</td>\n",
       "      <td>260</td>\n",
       "      <td>520</td>\n",
       "      <td>1000000</td>\n",
       "      <td>33.454209</td>\n",
       "      <td>46.959750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>55</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.00413</td>\n",
       "      <td>25.92</td>\n",
       "      <td>660</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1000000</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>1000000</td>\n",
       "      <td>38.147150</td>\n",
       "      <td>91.961300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>55</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.00222</td>\n",
       "      <td>27.27</td>\n",
       "      <td>665</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1000000</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>1000000</td>\n",
       "      <td>39.483009</td>\n",
       "      <td>93.792200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>55</td>\n",
       "      <td>-12</td>\n",
       "      <td>0.00173</td>\n",
       "      <td>27.79</td>\n",
       "      <td>670</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1000000</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>1000000</td>\n",
       "      <td>40.066968</td>\n",
       "      <td>94.807300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   min_output  max_output  status        a      b     c  t_min_down  t_min_up  \\\n",
       "0         150         455      96  0.00048  16.19  1000          96        96   \n",
       "1         150         455      96  0.00031  17.26   970          96        96   \n",
       "2          20         130     -60  0.00200  16.60   700          60        60   \n",
       "3          20         130     -60  0.00211  16.50   680          60        60   \n",
       "4          25         162     -72  0.00398  19.70   450          72        72   \n",
       "5          20          80     -36  0.00712  22.26   370          36        36   \n",
       "6          25          85     -36  0.00079  27.74   480          36        36   \n",
       "7          10          55     -12  0.00413  25.92   660          12        12   \n",
       "8          10          55     -12  0.00222  27.27   665          12        12   \n",
       "9          10          55     -12  0.00173  27.79   670          12        12   \n",
       "\n",
       "   t_max_up  hot_cost  cold_cost  cold_hrs  min_fuel_cost  max_cost_per_mwh  \n",
       "0   1000000      4500       9000   1000000      18.606202         22.928667  \n",
       "1   1000000      5000      10000   1000000      19.532918         23.773167  \n",
       "2   1000000       550       1100   1000000      22.244615         51.640000  \n",
       "3   1000000       560       1120   1000000      22.005069         50.542200  \n",
       "4   1000000       900       1800   1000000      23.122538         37.799500  \n",
       "5   1000000       170        340   1000000      27.454600         40.902400  \n",
       "6   1000000       260        520   1000000      33.454209         46.959750  \n",
       "7   1000000        30         60   1000000      38.147150         91.961300  \n",
       "8   1000000        30         60   1000000      39.483009         93.792200  \n",
       "9   1000000        30         60   1000000      40.066968         94.807300  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.gen_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-blanket",
   "metadata": {},
   "source": [
    "When we set `num_gen < 10`, RL4UC takes a subset of these generators by default. When using more than 10 generators, we use duplicates of these generators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-accordance",
   "metadata": {},
   "source": [
    "When making an environment from a json, we need to specify the environment's 'name', that is the prefix to '.json'. As mentioned, this must be in the directory `data/envs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "first-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl4uc.environment import make_env_from_json\n",
    "\n",
    "env = make_env_from_json('20gen_carbon1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-ethics",
   "metadata": {},
   "source": [
    "The preset environments are aimed at better defining benchmarks that people can compare performance on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-drain",
   "metadata": {},
   "source": [
    "### Training and Testing Modes\n",
    "\n",
    "Environments have two modes: train and test. Training mode is designed for when we want to sample from a **range of episodes**, specified in `env.profiles_df`. In training mode, when we 'reset' with `env.reset()` the environment samples a new episode from `env.profiles_df`. This makes it useful for training agents. In test mode, we are considering the case where `env.profiles_df` is **a single test episode**, and we want to evaluate the performance. Typically the test problem will not have been observed in training.\n",
    "\n",
    "When making an environment in training mode (e.g. `make_env(mode='train')`), the profiles in `data/train_data_10gen.csv` are automatically scaled to the number of generators and available as episodes. These profile are based on data from the GB power system. By contrast, when using a test environment you **must** specify a test profile.\n",
    "\n",
    "Some other differences between training and testing:\n",
    "\n",
    "- In train mode, states are terminal if there is lost load. In testing the episode continues. \n",
    "- In train mode, resetting the environment randomly initialises the generator up/down times. In test mode they are initialised to `env.gen_info.status`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-tuner",
   "metadata": {},
   "source": [
    "## Interacting with the environment\n",
    "\n",
    "RL4UC **roughly** follows the OpenAI Gym API. In other words, we act on the environment by executing `env.step(action)`, which returns an observation, reward and an indicator of whether the state is terminal. (Currently the 'info' return is not implemented). We can reset the environment to begin a new episode with `env.reset()`. **An action is a binary sequence of length `env.num_gen`, where 1 indicates turning the generator on, and 2 indicates turning it off.**\n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enclosed-request",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -7835.415966447421\n",
      "Done: False\n"
     ]
    }
   ],
   "source": [
    "env = make_env_from_json('5gen') # make the environment\n",
    "obs = env.reset() # reset the environment\n",
    "action = np.array([1,1,1,1,0]) # turn all but the last generator on\n",
    "obs, reward, done = env.step(action) # act on the environment\n",
    "\n",
    "print(\"Reward: {}\".format(reward))\n",
    "print(\"Done: {}\".format(done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-custom",
   "metadata": {},
   "source": [
    "We can see the reward achieved by the action. In addition, we can see whether the environment is 'done': that is, whether it has reached a terminal state. \n",
    "\n",
    "**NOTE: currently (and counter-intuitively) you can still continue to act on an environment after it has reached a terminal state. When training, you should embed each episode in a while loop, that checks for if environment is done (as in the next section).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-heating",
   "metadata": {},
   "source": [
    "### States\n",
    "\n",
    "States are comprised predominantly of the following components: generator up/down times, demand forecast, wind forecast, timestep. In addition are the forecast errors, produced by the ARMA processes. **Note: while the forecast errors are returned in the observation, you may want to consider whether the agent *should* observe this in a particular problem setting. If you are training an agent to solve the day-ahead problem, it probably *shouldn't* observe them, as that won't be possible when it comes to testing!**\n",
    "\n",
    "Let's see the observed state from the code block above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "operating-buffer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': array([ 1, 11, 13,  7, -1]),\n",
       " 'demand_forecast': array([516.42147491, 519.11935418, 522.06381382, 518.95980218,\n",
       "        516.24741818, 514.40531782, 507.93620945, 503.00460218,\n",
       "        498.59516509, 499.53797236, 508.21179927, 517.161216  ,\n",
       "        560.240256  , 607.03250618, 652.73690182, 671.82512291,\n",
       "        694.423488  , 704.37373091, 708.783168  , 710.39319273,\n",
       "        709.07326255, 704.59130182, 695.98999855, 694.69907782,\n",
       "        690.36216436, 685.41605236, 692.63940655, 692.37832145,\n",
       "        696.94731055, 699.34059055, 701.93693673, 715.33930473,\n",
       "        722.53364945, 744.16019782, 767.78839855, 787.97897891,\n",
       "        789.13935709, 781.90149818, 769.86257455, 753.39970909,\n",
       "        729.42339491, 700.06582691, 667.15460073, 632.32875055,\n",
       "        601.86882327, 571.64097164, 547.14248727, 534.49436509]),\n",
       " 'demand_errors': array([-13.95413774,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ]),\n",
       " 'wind_forecast': array([2.90985226e+02, 2.98426773e+02, 2.51745001e+02, 2.17203550e+02,\n",
       "        1.59758309e+02, 1.73319189e+02, 1.96419672e+02, 2.56203373e+02,\n",
       "        2.50783392e+02, 1.71428752e+02, 2.08942450e+02, 1.96518018e+02,\n",
       "        1.85098905e+02, 1.40121805e+02, 1.07285026e+02, 1.68467432e+02,\n",
       "        1.75460955e+02, 1.78728242e+02, 2.25650416e+02, 1.81012065e+02,\n",
       "        1.21938644e+02, 1.33281265e+02, 1.43148690e+02, 1.00160373e+02,\n",
       "        8.41954699e+01, 8.00102830e+01, 5.80681022e+01, 3.48801999e+01,\n",
       "        3.32738749e+01, 4.00816332e+01, 3.04327558e+01, 2.18875440e+01,\n",
       "        1.32330583e+01, 1.44459976e+01, 1.65003452e+01, 1.23370131e+01,\n",
       "        5.24514284e+00, 1.64784904e+01, 4.60042736e+00, 7.09187021e+00,\n",
       "        1.46208357e+01, 1.12989119e+01, 2.40402380e-01, 3.42027023e+00,\n",
       "        1.05230678e+01, 2.39637463e+01, 1.71341333e+01, 4.27260594e+00]),\n",
       " 'wind_errors': array([-13.25270849,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ]),\n",
       " 'timestep': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-sauce",
   "metadata": {},
   "source": [
    "The `status` component gives the up/down times in decision periods. Remember that the generators have minimum up/down time constraints, so the up/down times are a very important component of the state vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-audience",
   "metadata": {},
   "source": [
    "## Training an Agent\n",
    "\n",
    "Now we'll look at how to train an agent. \n",
    "\n",
    "In this case we'll look at a simple Q-learning agent. Needless to say, this is a pretty simple solution that could be vastly improved.\n",
    "\n",
    "**Note: this requires Pytorch to run yourself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "attached-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class QAgent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(QAgent, self).__init__()\n",
    "        self.num_gen = env.num_gen\n",
    "        \n",
    "        self.num_nodes = 32\n",
    "        self.gamma = 0.99\n",
    "        self.activation = torch.tanh\n",
    "        \n",
    "        # There are 2N output nodes, corresponding to ON/OFF for each generator\n",
    "        self.n_out = 2*self.num_gen\n",
    "        \n",
    "        self.obs_size = self.process_observation(env.reset()).size\n",
    "        \n",
    "        self.in_layer = nn.Linear(self.obs_size, self.num_nodes)\n",
    "        self.out_layer = nn.Linear(self.num_nodes, self.n_out) \n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=3e-04)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "                \n",
    "    def process_observation(self, obs):\n",
    "        \"\"\"\n",
    "        Process an observation into a numpy array.\n",
    "        \n",
    "        Observations are given as dictionaries, which is not very convenient\n",
    "        for function approximation. Here we take just the generator up/down times\n",
    "        and the timestep.\n",
    "        \n",
    "        Customise this!\n",
    "        \"\"\"\n",
    "        obs_new = np.concatenate((obs['status'], [obs['timestep']]))\n",
    "        return obs_new\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        x = torch.as_tensor(obs).float()\n",
    "        x = self.activation(self.in_layer(x))\n",
    "        return self.out_layer(x)\n",
    "        \n",
    "    def act(self, obs):\n",
    "        \"\"\"\n",
    "        Agent always acts greedily w.r.t Q-values!\n",
    "        \"\"\"\n",
    "        processed_obs = self.process_observation(obs)\n",
    "\n",
    "        q_values = self.forward(processed_obs)\n",
    "        q_values = q_values.reshape(self.num_gen, 2)\n",
    "        action = q_values.argmax(axis=1).detach().numpy()\n",
    "        \n",
    "        return action, processed_obs\n",
    "    \n",
    "    def update(self, memory, batch_size=None):\n",
    "        \n",
    "        if batch_size == None:\n",
    "            batch_size = memory.capacity\n",
    "        \n",
    "        data = memory.sample(batch_size)\n",
    "        \n",
    "        qs = self.forward(data['obs']).reshape(batch_size, self.num_gen, 2)\n",
    "        \n",
    "        # A bit of complicated indexing here! \n",
    "        # We are using the actions [batch_size, num_gen] to index Q-values\n",
    "        # which have shape [batch_size, num_gen, 2]\n",
    "        m,n = data['act'].shape\n",
    "        I,J = np.ogrid[:m,:n]\n",
    "        qs = qs[I, J, data['act']]\n",
    "        \n",
    "        next_qs = self.forward(data['next_obs']).reshape(batch_size, self.num_gen, 2)\n",
    "        next_acts = next_qs.argmax(axis=2).detach().numpy()\n",
    "        \n",
    "        # The same complicated indexing! \n",
    "        m,n = next_acts.shape\n",
    "        I,J = np.ogrid[:m,:n]\n",
    "        next_qs = next_qs[I, J, next_acts]\n",
    "        \n",
    "        # Recasting rewards into the same shape as next_qs\n",
    "        m,n = next_qs.shape\n",
    "        rews = np.broadcast_to(data['rew'], (self.num_gen,batch_size)).T\n",
    "        rews = torch.as_tensor(rews).float()\n",
    "\n",
    "        td_target = rews + self.gamma * next_qs\n",
    "                \n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(qs, td_target)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity, obs_size, act_dim):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.obs_size = obs_size\n",
    "        self.act_dim = act_dim \n",
    "        \n",
    "        self.act_buf = np.zeros((self.capacity, self.act_dim))\n",
    "        self.obs_buf = np.zeros((self.capacity, self.obs_size))\n",
    "        self.rew_buf = np.zeros(self.capacity)\n",
    "        self.next_obs_buf = np.zeros((self.capacity, self.obs_size))\n",
    "        \n",
    "        self.num_used = 0\n",
    "        \n",
    "    def store(self, obs, action, reward, next_obs):\n",
    "        \"\"\"Store a transition in the memory\"\"\"\n",
    "        idx = self.num_used % self.capacity\n",
    "        \n",
    "        self.act_buf[idx] = action\n",
    "        self.obs_buf[idx] = obs\n",
    "        self.rew_buf[idx] = reward\n",
    "        self.next_obs_buf[idx] = next_obs\n",
    "        \n",
    "        self.num_used += 1\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(self.capacity), size=batch_size, replace=False)\n",
    "        \n",
    "        data = {'act': self.act_buf[idx],\n",
    "                'obs': self.obs_buf[idx],\n",
    "                'rew': self.rew_buf[idx],\n",
    "                'next_obs': self.next_obs_buf[idx]}\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    def is_full(self):\n",
    "        return (self.num_used >= self.capacity)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.num_used = 0 \n",
    "        \n",
    "def train():\n",
    "    \n",
    "    MEMORY_SIZE = 200\n",
    "    N_EPOCHS = 500\n",
    "    \n",
    "    env = make_env_from_json('5gen')\n",
    "    agent = QAgent(env)\n",
    "    memory = ReplayMemory(MEMORY_SIZE, agent.obs_size, env.num_gen)\n",
    "    \n",
    "    log = {'mean_timesteps': [],\n",
    "           'mean_reward': []}\n",
    "    \n",
    "    for i in range(N_EPOCHS):\n",
    "        if i % 10 == 0:\n",
    "            print(\"Epoch {}\".format(i))\n",
    "        epoch_timesteps = []\n",
    "        epoch_rewards = []\n",
    "        while memory.is_full() == False:\n",
    "            done = False\n",
    "            obs = env.reset()\n",
    "            timesteps = 0\n",
    "            while not done: \n",
    "                action, processed_obs = agent.act(obs)\n",
    "                next_obs, reward, done = env.step(action)\n",
    "                \n",
    "                next_obs_processed = agent.process_observation(next_obs)\n",
    "                \n",
    "                memory.store(processed_obs, action, reward, next_obs_processed)\n",
    "                \n",
    "                obs = next_obs\n",
    "                \n",
    "                if memory.is_full():\n",
    "                    break\n",
    "                \n",
    "                timesteps += 1\n",
    "                if done:\n",
    "                    epoch_rewards.append(reward)\n",
    "                    epoch_timesteps.append(timesteps)\n",
    "                    \n",
    "        log['mean_timesteps'].append(np.mean(epoch_timesteps))\n",
    "        log['mean_reward'].append(np.mean(epoch_rewards))\n",
    "        \n",
    "        agent.update(memory)\n",
    "        memory.reset()\n",
    "                    \n",
    "    return agent, log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "existing-highland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 10\n",
      "Epoch 20\n",
      "Epoch 30\n",
      "Epoch 40\n",
      "Epoch 50\n",
      "Epoch 60\n",
      "Epoch 70\n",
      "Epoch 80\n",
      "Epoch 90\n",
      "Epoch 100\n",
      "Epoch 110\n",
      "Epoch 120\n",
      "Epoch 130\n",
      "Epoch 140\n",
      "Epoch 150\n",
      "Epoch 160\n",
      "Epoch 170\n",
      "Epoch 180\n",
      "Epoch 190\n",
      "Epoch 200\n",
      "Epoch 210\n",
      "Epoch 220\n",
      "Epoch 230\n",
      "Epoch 240\n",
      "Epoch 250\n",
      "Epoch 260\n",
      "Epoch 270\n",
      "Epoch 280\n",
      "Epoch 290\n",
      "Epoch 300\n",
      "Epoch 310\n",
      "Epoch 320\n",
      "Epoch 330\n",
      "Epoch 340\n",
      "Epoch 350\n",
      "Epoch 360\n",
      "Epoch 370\n",
      "Epoch 380\n",
      "Epoch 390\n",
      "Epoch 400\n",
      "Epoch 410\n",
      "Epoch 420\n",
      "Epoch 430\n",
      "Epoch 440\n",
      "Epoch 450\n",
      "Epoch 460\n",
      "Epoch 470\n",
      "Epoch 480\n",
      "Epoch 490\n"
     ]
    }
   ],
   "source": [
    "agent, log = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-miami",
   "metadata": {},
   "source": [
    "And now let's see if the agent managed to improve its performance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "divided-sarah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "pd.Series(log['mean_reward']).rolling(50).mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-shift",
   "metadata": {},
   "source": [
    "Some things you may want to consider when writing an agent:\n",
    "\n",
    "- The action space is combinatorial\n",
    "- Illegal actions are corrected by the environment\n",
    "- Observations and rewards are unnormalised by default"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
